{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4392e2fb-25e2-436c-a507-a42113d76595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Sagemaker notebooks may require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Running on public URL: https://4dd349f7d3a4fa3175.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4dd349f7d3a4fa3175.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3192fb57d79d411b8eff76d40e121361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_api=True\n",
    "SSD_1B=False\n",
    "\n",
    "\n",
    "import os\n",
    "# Use GPU\n",
    "gpu_info = os.popen('nvidia-smi').read()\n",
    "if 'failed' in gpu_info:\n",
    "    print('Not connected to a GPU')\n",
    "    is_gpu = False\n",
    "else:\n",
    "    print(gpu_info)\n",
    "    is_gpu = True\n",
    "print(is_gpu)\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "def check_enviroment():\n",
    "    try:\n",
    "        import torch\n",
    "        print(\"Enviroment is already installed.\")\n",
    "    except ImportError:\n",
    "        print(\"Enviroment not found. Installing...\")\n",
    "        # Install requirements from requirements.txt\n",
    "        os.system('pip install -r requirements.txt')\n",
    "        # Install gradio version 3.48.0\n",
    "        os.system('pip install gradio==3.39.0')\n",
    "        # Install python-dotenv\n",
    "        os.system('pip install python-dotenv')\n",
    "        # Clear the output\n",
    "        clear_output()        \n",
    "        \n",
    "        print(\"Enviroment installed successfully.\")\n",
    "\n",
    "# Call the function to check and install Packages if necessary\n",
    "check_enviroment()\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import PIL\n",
    "import base64\n",
    "import io\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel, DiffusionPipeline, LCMScheduler\n",
    "\n",
    "# SDXL\n",
    "from diffusers import UNet2DConditionModel, DiffusionPipeline, LCMScheduler\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "model_path = os.path.join(current_dir)\n",
    "# Set the cache path\n",
    "cache_path = os.path.join(current_dir, \"cache\")\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "MAX_IMAGE_SIZE = int(os.getenv('MAX_IMAGE_SIZE', '1024'))\n",
    "SECRET_TOKEN = os.getenv('SECRET_TOKEN', 'default_secret')\n",
    "\n",
    "# Uncomment the following line if you are using PyTorch 1.10 or later\n",
    "# os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "if is_gpu:\n",
    "    # Uncomment the following line if you want to enable CUDA launch blocking\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "else:\n",
    "    # Uncomment the following line if you want to use CPU instead of GPU\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "model_path = os.path.join(current_dir)\n",
    "\n",
    "# Set the cache path\n",
    "cache_path = os.path.join(current_dir, \"cache\")\n",
    "\n",
    "if not SSD_1B:\n",
    "\n",
    "    unet = UNet2DConditionModel.from_pretrained(\"latent-consistency/lcm-sdxl\",\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                variant=\"fp16\",\n",
    "                                                cache_dir=cache_path)\n",
    "    pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                                             unet=unet,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             variant=\"fp16\",\n",
    "                                             cache_dir=cache_path)\n",
    "\n",
    "    pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "    if torch.cuda.is_available():\n",
    "        pipe.to('cuda')\n",
    "else:\n",
    "    # SSD-1B\n",
    "    from diffusers import LCMScheduler, AutoPipelineForText2Image        \n",
    "\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(\"segmind/SSD-1B\",\n",
    "                                                     torch_dtype=torch.float16, \n",
    "                                                     variant=\"fp16\",\n",
    "                                                     cache_dir=cache_path)\n",
    "    pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "    if torch.cuda.is_available():\n",
    "        pipe.to('cuda')\n",
    "\n",
    "\n",
    "    # load and fuse\n",
    "    pipe.load_lora_weights(\"latent-consistency/lcm-lora-ssd-1b\")\n",
    "    pipe.fuse_lora()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate(prompt: str,\n",
    "             negative_prompt: str = '',\n",
    "             seed: int = 0,\n",
    "             width: int = 1024,\n",
    "             height: int = 1024,\n",
    "             guidance_scale: float = 0.0,\n",
    "             num_inference_steps: int = 4,\n",
    "             secret_token: str = '') -> PIL.Image.Image:\n",
    "    if secret_token != SECRET_TOKEN:\n",
    "        raise gr.Error(\n",
    "            f'Invalid secret token. Please fork the original space if you want to use it for yourself.')\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    image = pipe(prompt=prompt,\n",
    "                 negative_prompt=negative_prompt,\n",
    "                 width=width,\n",
    "                 height=height,\n",
    "                 guidance_scale=guidance_scale,\n",
    "                 num_inference_steps=num_inference_steps,\n",
    "                 generator=generator,\n",
    "                 output_type='pil').images[0]\n",
    "    return image\n",
    "clear_output()\n",
    "\n",
    "from IPython.display import display\n",
    "def generate_image(prompt=\"A beautiful and sexy girl\"):\n",
    "    # Generate the image using the prompt\n",
    "    generated_image = generate(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"\",\n",
    "        seed=0,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        guidance_scale=0.0,\n",
    "        num_inference_steps=4,\n",
    "        secret_token='default_secret',  # Replace with your secret token\n",
    "    )\n",
    "    # Display the image in the Jupyter Notebook\n",
    "    display(generated_image)\n",
    "\n",
    "if not run_api:\n",
    "    secret_token = gr.Text(\n",
    "        label='Secret Token',\n",
    "        max_lines=1,\n",
    "        placeholder='Enter your secret token',\n",
    "    )\n",
    "    prompt = gr.Text(\n",
    "        label='Prompt',\n",
    "        show_label=False,\n",
    "        max_lines=1,\n",
    "        placeholder='Enter your prompt',\n",
    "        container=False,\n",
    "    )\n",
    "    result = gr.Image(label='Result', show_label=False)\n",
    "    negative_prompt = gr.Text(\n",
    "        label='Negative prompt',\n",
    "        max_lines=1,\n",
    "        placeholder='Enter a negative prompt',\n",
    "        visible=True,\n",
    "    )\n",
    "    seed = gr.Slider(label='Seed',\n",
    "                     minimum=0,\n",
    "                     maximum=MAX_SEED,\n",
    "                     step=1,\n",
    "                     value=0)\n",
    "\n",
    "    width = gr.Slider(\n",
    "        label='Width',\n",
    "        minimum=256,\n",
    "        maximum=MAX_IMAGE_SIZE,\n",
    "        step=32,\n",
    "        value=1024,\n",
    "    )\n",
    "    height = gr.Slider(\n",
    "        label='Height',\n",
    "        minimum=256,\n",
    "        maximum=MAX_IMAGE_SIZE,\n",
    "        step=32,\n",
    "        value=1024,\n",
    "    )\n",
    "    guidance_scale = gr.Slider(\n",
    "        label='Guidance scale',\n",
    "        minimum=0,\n",
    "        maximum=2,\n",
    "        step=0.1,\n",
    "        value=0.0\n",
    "    )\n",
    "    num_inference_steps = gr.Slider(\n",
    "        label='Number of inference steps',\n",
    "        minimum=1,\n",
    "        maximum=8,\n",
    "        step=1,\n",
    "        value=4\n",
    "    )\n",
    "    inputs = [\n",
    "        prompt,\n",
    "        negative_prompt,\n",
    "        seed,\n",
    "        width,\n",
    "        height,\n",
    "        guidance_scale,\n",
    "        num_inference_steps,\n",
    "        secret_token,\n",
    "    ]\n",
    "    iface = gr.Interface(fn=generate,\n",
    "                         inputs=inputs,\n",
    "                         outputs=result,\n",
    "                         title='Image Generator',\n",
    "                         description='Generate images based on prompts.')\n",
    "\n",
    "    iface.launch()\n",
    "    \n",
    "\n",
    "if run_api:\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.HTML(\"\"\"\n",
    "        <div style=\"z-index: 100; position: fixed; top: 0px; right: 0px; left: 0px; bottom: 0px; width: 100%; height: 100%; background: white; display: flex; align-items: center; justify-content: center; color: black;\">\n",
    "            <div style=\"text-align: center; color: black;\">\n",
    "                <p style=\"color: black;\">This space is a REST API to programmatically generate images using LCM LoRA SSD-1B.</p>\n",
    "                <p style=\"color: black;\">It is not meant to be directly used through a user interface, but using code and an access key.</p>\n",
    "            </div>\n",
    "        </div>\"\"\")\n",
    "        secret_token = gr.Text(\n",
    "            label='Secret Token',\n",
    "            max_lines=1,\n",
    "            placeholder='Enter your secret token',\n",
    "        )\n",
    "        prompt = gr.Text(\n",
    "            label='Prompt',\n",
    "            show_label=False,\n",
    "            max_lines=1,\n",
    "            placeholder='Enter your prompt',\n",
    "            container=False,\n",
    "        )\n",
    "        result = gr.Image(label='Result', show_label=False)\n",
    "        negative_prompt = gr.Text(\n",
    "            label='Negative prompt',\n",
    "            max_lines=1,\n",
    "            placeholder='Enter a negative prompt',\n",
    "            visible=True,\n",
    "        )\n",
    "        seed = gr.Slider(label='Seed',\n",
    "                        minimum=0,\n",
    "                        maximum=MAX_SEED,\n",
    "                        step=1,\n",
    "                        value=0)\n",
    "\n",
    "        width = gr.Slider(\n",
    "            label='Width',\n",
    "            minimum=256,\n",
    "            maximum=MAX_IMAGE_SIZE,\n",
    "            step=32,\n",
    "            value=1024,\n",
    "        )\n",
    "        height = gr.Slider(\n",
    "            label='Height',\n",
    "            minimum=256,\n",
    "            maximum=MAX_IMAGE_SIZE,\n",
    "            step=32,\n",
    "            value=1024,\n",
    "        )\n",
    "        guidance_scale = gr.Slider(\n",
    "            label='Guidance scale',\n",
    "            minimum=0,\n",
    "            maximum=2,\n",
    "            step=0.1,\n",
    "            value=0.0)\n",
    "        num_inference_steps = gr.Slider(\n",
    "            label='Number of inference steps',\n",
    "            minimum=1,\n",
    "            maximum=8,\n",
    "            step=1,\n",
    "            value=4)\n",
    "\n",
    "        inputs = [\n",
    "            prompt,\n",
    "            negative_prompt,\n",
    "            seed,\n",
    "            width,\n",
    "            height,\n",
    "            guidance_scale,\n",
    "            num_inference_steps,\n",
    "            secret_token,\n",
    "        ]\n",
    "        prompt.submit(\n",
    "            fn=generate,\n",
    "            inputs=inputs,\n",
    "            outputs=result,\n",
    "            api_name='run',\n",
    "        )\n",
    "\n",
    "    #demo.queue(max_size=32).launch()\n",
    "    # Launch the Gradio app with multiple workers and debug mode enabled\n",
    "    demo.queue(max_size=32).launch(debug=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccf3f5-3352-468f-ae43-0fcbaa0ac168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
